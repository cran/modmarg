<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Alex Gold, Nat Olin, Annie Wang" />

<meta name="date" content="2017-08-13" />

<title>What is the Delta Method?</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">What is the Delta Method?</h1>
<h4 class="author"><em>Alex Gold, Nat Olin, Annie Wang</em></h4>
<h4 class="date"><em>2017-08-13</em></h4>



<p>There are a number of ways to compute the standard errors for the <a href="usage.html">margins</a> of a regression. It might be possible to derive a probability density function for the margin itself, but that’s perhaps a huge pain and might not even exist. It is also possible to use simulation or bootstrapping to create standard errors for the margin.</p>
<p>In this package, we follow <a href="https://www.stata.com/help.cgi?margins">Stata’s margins command</a> and use the delta method, which is a semi-parametric method that takes advantage of a closed-form solution to <span class="math inline">\(\frac{d(\text{link}^{-1}(X \beta))}{d(X \beta)}\)</span> to improve computational time relative to simulation or bootstrap methods.</p>
<div id="the-delta-method" class="section level1">
<h1>The Delta Method</h1>
<p>The delta method is a general method for deriving the variance of a function of asymptotically normal random variables with known variance. In this case, the delta method takes advantage of the fact that the margin is (usually) an infinitely differentiable function of the data, <span class="math inline">\(X\)</span>, and the vector of <span class="math inline">\(\beta\)</span>s to derive a closed-form solution for the standard errors of the margin.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>In particular, the delta method uses a <a href="https://en.wikipedia.org/wiki/Taylor_series">Taylor series</a> expansion of the inverse link function of the regression to approximate the margin in the neighborhood of <span class="math inline">\(X\)</span> and the <span class="math inline">\(\beta\)</span>s and derive variation near that point.</p>
<div id="a-reminder-about-taylor-series" class="section level2">
<h2>A Reminder about Taylor Series</h2>
<p>The <a href="https://en.wikipedia.org/wiki/Taylor_series">Taylor expansion</a> is a useful tool because it allows us to restate a differentiable function, <span class="math inline">\(G(x)\)</span>, in terms of (an infinite sum of) the derivatives of <span class="math inline">\(G(x)\)</span>. To be more precise, an infinitely differentiable <span class="math inline">\(G(x)\)</span> evaluated at <span class="math inline">\(a\)</span> can be written as</p>
<p><span class="math display">\[G(x) = G(a) + \frac{G'(a)}{1!}(x - a) + \frac{G''(a)}{2!}(x-a)^2 + 
\frac{G'''(a)}{3!}(x-a)^3 + \dots\]</span></p>
<p>If we cut off the expansion after some number of terms (two is commmon), we can get a useful approximation of <span class="math inline">\(G(x)\)</span>.</p>
</div>
<div id="taylor-series-and-the-delta-method" class="section level2">
<h2>Taylor Series and the Delta Method</h2>
<p>In the case of predicted margins (levels), where the regression model has <a href="https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function">link function</a> <span class="math inline">\(\text{link}\)</span><a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>, the column vector of predicted levels <span class="math inline">\(P_m\)</span> at covariates <span class="math inline">\(X_1\)</span> is</p>
<p><span class="math display">\[P_m(X_1 \beta) = \text{link}^{-1}(X_1 \beta)\]</span></p>
<p>The predicted effects <span class="math inline">\(P_e\)</span> of that same regression is a function of <span class="math inline">\(X \beta\)</span> such that</p>
<p><span class="math display">\[P_e(X_1 \beta) = \frac{d(\text{link}^{-1}(X_1 \beta))}{d(X \beta)}\]</span></p>
<p>Depending on whether the effect is over a continuous or categorical variable, this may be an actual derivative (the instantaneous rate of change) or the subtraction of <span class="math inline">\(P(X \beta)\)</span> calculated at one value of <span class="math inline">\(X\)</span> from another (the first difference).</p>
<p>Using the Taylor expansion, we can approximate <span class="math inline">\(P\)</span>, an arbitrary function of the random variable <span class="math inline">\(X \beta\)</span> around the point <span class="math inline">\(X_1 \beta\)</span>, as<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<p><span class="math display">\[P(X \beta) = P(X_1 \beta) + \frac{d(P(X_1 \beta))}{d(X\beta)}(X\beta - X_1 \beta)\]</span></p>
<p>Now we can substitute for the different margins we’ll care about. For predicted levels, the Taylor expansion is</p>
<p><span class="math display">\[P_m(X \beta) = \text{link}^{-1}(X_1 \beta) + 
\frac{d(\text{link}^{-1}(X_1 \beta))}{d(X \beta)}(X\beta - X_1 \beta)\]</span></p>
<p>For the predicted effects of categorical variables, we are trying to estimate the effects at <span class="math inline">\(P_e(X_1 \beta - X_2 \beta)\)</span>, which gives us</p>
<p><span class="math display">\[\begin{aligned}
P_e(X \beta) &amp;= \text{link}^{-1} (X_1 \beta - X_2 \beta) + 
\frac{d(\text{link}^{-1}(X_1 \beta - X_2 \beta))}{d(X \beta)}(X\beta - (X_1 \beta - X_2 \beta)) \\
% &amp;= \text{link}^{-1} (X_1 \beta) - \text{link}^{-1} (X_2 \beta) + \frac{d(\text{link}^{-1})}{d(X \beta)}(X_1) - \frac{d(\text{link}^{-1})}{d(X \beta)}(X_2)
\end{aligned}
\]</span></p>
<p>For the predicted effects of continuous variables, the marginal effect is a derivative, so</p>
<p><span class="math display">\[\begin{aligned}
P_e(X_1 \beta) &amp;= \frac{d(\text{link}^{-1}(X_1 \beta))}{d(X \beta)} + 
\frac{d^2(\text{link}^{-1}(X_1 \beta))}{d(X \beta)}\left(\frac{d(\text{link}^{-1}(X \beta))}{d(X \beta)} - \frac{d(\text{link}^{-1}(X_1 \beta))}{d(X \beta)}\right)
\end{aligned}
\]</span></p>
</div>
</div>
<div id="how-does-the-delta-method-work" class="section level1">
<h1>How does the Delta Method Work?</h1>
<p>The equations above describe how to apprxoimate predicted levels or effects, but why not just calculate our estimate <span class="math inline">\(P(X\beta)\)</span> directly?</p>
<p>Well, we can do that for the point estimate, but also want to calculate errors on that estimate, and the variance of <span class="math inline">\(P(X\beta)\)</span> isn’t known. Fortunately, we <em>can</em> calculate the variance of the approximations above. <a href="https://en.wikipedia.org/wiki/Delta_method#Multivariate_delta_method">Wikipedia contains a really nice derivation</a> of the multivariate delta method:</p>
<p><span class="math display">\[\text{Var}[P(X \beta)] = \text{Var}\left[P(X_1 \beta) + \frac{d(P(X_1 \beta))}{d(X\beta)}(X\beta - X_1 \beta)\right]\]</span></p>
<p>Because <span class="math inline">\(X_1\beta\)</span> is a known point, it has variance of zero, so this simplifies to</p>
<p><span class="math display">\[\text{Var}[P(X \beta)] = \text{Var}\left[\frac{d(P(X_1 \beta))}{d(X\beta)} \cdot X\beta\right]\]</span></p>
<p>The first term in the variance is the vector of partial derivatives of our estimator, also known as our jacobian matrix.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> For any predicted level indexed by <span class="math inline">\(i\)</span> in a regression, the <span class="math inline">\(i,j\)</span>th element of the jacobian will be the derivative of predicted level <span class="math inline">\(i\)</span> with respect to regressor <span class="math inline">\(j\)</span>. Those are fixed quantities. We also know the variance of <span class="math inline">\(X\beta\)</span>, since that’s our variance-covariance matrix <span class="math inline">\(V\)</span>.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p>
<p>So we want the variance of a random variable multiplied by a fixed matrix, which we can find.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></p>
<p><span class="math display">\[\text{Var}[P(X \beta)] = \left(\frac{d(P(X_1 \beta))}{d(X\beta)}\right)^T \ V \left(\frac{d(P(X_1 \beta))}{d(X\beta)}\right)\]</span></p>
<p>So practically speaking, to get our variance, we’ll pre- and post-multiply the partial derivatives of the inverse link function by the original variance-covariance matrix from the regression.</p>
<p>In short,</p>
<ol style="list-style-type: decimal">
<li><p>Calculate the jacobian matrix of the inverse link function of <span class="math inline">\(X \beta\)</span>, <span class="math inline">\(J\)</span>.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a></p></li>
<li><p>Get the variance-covariance matrix, <span class="math inline">\(V\)</span>, from the regression output, or calculate it some other way.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p></li>
<li><p>Sandwich multiply the matrices: <span class="math inline">\(J^{T} V J\)</span>. You’ll end up with a <span class="math inline">\(k \times 1\)</span> matrix for the <span class="math inline">\(k\)</span> predicted levels/effects.</p></li>
</ol>
</div>
<div id="example" class="section level1">
<h1>Example</h1>
<p>Say we’re fitting a logistic regression using the <code>margex</code> data, and we’re interested in the predicted outcome for different treatment groups:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(modmarg)
<span class="kw">data</span>(margex)
lg &lt;-<span class="st"> </span><span class="kw">glm</span>(outcome ~<span class="st"> </span>treatment *<span class="st"> </span>age, <span class="dt">data =</span> margex, <span class="dt">family =</span> <span class="st">'binomial'</span>)
<span class="kw">summary</span>(lg)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = outcome ~ treatment * age, family = &quot;binomial&quot;, 
##     data = margex)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.3200  -0.6033  -0.3212  -0.1580   2.9628  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   -7.03092    0.50247 -13.993   &lt;2e-16 ***
## treatment      1.35170    0.62208   2.173   0.0298 *  
## age            0.11060    0.01069  10.347   &lt;2e-16 ***
## treatment:age -0.01046    0.01301  -0.804   0.4216    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2732.1  on 2999  degrees of freedom
## Residual deviance: 2169.4  on 2996  degrees of freedom
## AIC: 2177.4
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>where the <code>treatment</code> variable <span class="math inline">\(\tau \in {0, 1}\)</span>. What is the average outcome when <span class="math inline">\(\tau = 0\)</span> or <span class="math inline">\(\tau = 1\)</span>?</p>
<p>Based on these results alone, we don’t really know what the predicted outcome is for different treatment groups because we can’t directly interpret the coefficients: there’s an interaction term, and the effect of each covariate depends on the levels of the other covariates.</p>
<div id="point-estimates" class="section level2">
<h2>Point estimates</h2>
<p>To get an estimate of the average levels of the outcome for different values of treatment, we can set treatment to 0 or 1 for the entire dataset, generate predicted outcomes for the dataset, and average the results (in other words, what would the outcome have been, if everybody was in the control / treatment group?). The linear predictors of our model (in this case, setting <span class="math inline">\(\tau = t\)</span>), are given by: <span class="math display">\[
\hat{y}_i = \alpha + \beta_1 \cdot t + \beta_2 \cdot \text{age}_i + \beta_3 \cdot (t \cdot \text{age}_i)
\]</span> also known as <span class="math inline">\(\hat{y}_i = X_i\beta\)</span>. We can then transform the linear predictors to the scale of the outcome variable (predicted probabilities) with the inverse link function: <span class="math display">\[f(z) = \frac{1}{1 + \exp(-z)}\]</span> Finally, we can find the average predicted level, by taking the average of these predicted probabilities. Combining these, we have: <span class="math display">\[
P(X\beta) = \frac{1}{n} \sum_{i = 1}^n \frac{1}{1 + \exp(-X_i\beta)}
\]</span> Now let’s do this in R: set treatment to 0 or 1, generate linear predictors, transform them to predicted outcomes, and take the mean.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Extract the n x k matrix of data</span>
x &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(lg)

<span class="co"># Extract the coefficients from the model (a column vector with k entries)</span>
beta &lt;-<span class="st"> </span><span class="kw">matrix</span>(lg$coefficients)

<span class="co"># CONTROL:</span>

<span class="co"># Set treatment and treatment:age to 0 for all observations</span>
x[, <span class="st">&quot;treatment&quot;</span>] &lt;-<span class="st"> </span><span class="dv">0</span>
x[, <span class="st">'treatment:age'</span>] &lt;-<span class="st"> </span>x[, <span class="st">'treatment'</span>] *<span class="st"> </span>x[, <span class="st">'age'</span>]

<span class="co"># Get linear predictors</span>
pred_ctl &lt;-<span class="st"> </span>x %*%<span class="st"> </span>beta

<span class="co"># Apply the inverse link function to get predicted probabilities</span>
pp_ctl &lt;-<span class="st"> </span><span class="dv">1</span> /<span class="st"> </span>(<span class="dv">1</span> +<span class="st"> </span><span class="kw">exp</span>(-pred_ctl))

<span class="co"># Get the average predicted probability</span>
mean_pp_ctl &lt;-<span class="st"> </span><span class="kw">mean</span>(pp_ctl)

<span class="co"># TREATMENT:</span>

<span class="co"># Set treatment to 1 and treatment:age to age for all observations</span>
x[, <span class="st">&quot;treatment&quot;</span>] &lt;-<span class="st"> </span><span class="dv">1</span>
x[, <span class="st">'treatment:age'</span>] &lt;-<span class="st"> </span>x[, <span class="st">'treatment'</span>] *<span class="st"> </span>x[, <span class="st">'age'</span>]

<span class="co"># Get linear predictors</span>
pred_treat &lt;-<span class="st"> </span>x %*%<span class="st"> </span>beta

<span class="co"># Apply the inverse link function to get predicted probabilities</span>
pp_treat &lt;-<span class="st"> </span><span class="dv">1</span> /<span class="st"> </span>(<span class="dv">1</span> +<span class="st"> </span><span class="kw">exp</span>(-pred_treat))

<span class="co"># Get the average predicted probability</span>
mean_pp_treat &lt;-<span class="st"> </span><span class="kw">mean</span>(pp_treat)

<span class="co"># RESULTS:</span>

mean_pp_ctl</code></pre></div>
<pre><code>## [1] 0.1126685</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mean_pp_treat</code></pre></div>
<pre><code>## [1] 0.208375</code></pre>
</div>
<div id="variance" class="section level2">
<h2>Variance</h2>
<p>So far this is pretty straightforward, but we want to know the standard error of these estimates.</p>
<p>You might be tempted to just use the R command, <code>predict(model, newdata, se.fit = TRUE)</code>, fix treatment to 0 or 1, and find the average of the resulting standard errors. Don’t do that! You’ll average the standard errors of the predicted outcome of each observation in your dataset - which is <em>not</em> the same thing as finding the standard error for a specific estimate, which is what we’re looking for.</p>
<p>As explained above, we can approximate the variance of the predicted margins: using <span class="math inline">\(J\)</span> to represent the jacobian (the derivative of our transformation with respect to each <span class="math inline">\(\beta\)</span> parameter), then</p>
<p><span class="math display">\[\text{Var}(P(X\beta)) =  J V J^T\]</span></p>
<p>Going back to our function from before, let’s start with the coefficient on treatment, <span class="math inline">\(\beta_1\)</span>. First we just apply the quotient rule (and the chain rule in the last step):</p>
\begin{align*}
P(X\beta) &amp;= \frac{1}{n} \sum_{i = 1}^n \frac{1}{1 + \exp(-X_i\beta)}\\
\frac{\partial}{\partial \beta_1} P(X\beta) &amp;= \frac{\partial}{\partial \beta_1} \left[\frac{1}{n} \sum_{i = 1}^n \frac{1}{1 + \exp(-X_i\beta )}\right]\\
&amp;= \frac{1}{n} \sum_{i = 1}^n \frac{\partial}{\partial \beta_1} \left[\frac{1}{1 + \exp(-X_i\beta)}\right]\\
&amp;= \frac{1}{n} \sum_{i = 1}^n \frac{-\frac{\partial}{\partial \beta_1}[1 + \exp(-X_i\beta)]}{(1 + \exp(-X_i\beta))^2} \\
&amp;= \frac{1}{n} \sum_{i = 1}^n \frac{-\exp(-X_i\beta)}{(1 + \exp(-X_i\beta))^2} \cdot \frac{\partial}{\partial \beta_1} [-X_i\beta]
\end{align*}
<p>Coming up for air for a second: what’s the last term in that final expression?</p>
<p><span class="math inline">\(X_i\beta = \alpha + \beta_1 \tau_i + \beta_2 \text{age}_i + \beta_3 (\tau_i \cdot \text{age}_i)\)</span>, so the derivative of that with respect to <span class="math inline">\(\beta_1\)</span> is just <span class="math inline">\(\tau_i\)</span>. Therefore,</p>
\begin{align*}
\frac{\partial}{\partial \beta_1} P(X\beta) 
&amp;= \frac{1}{n} \sum_{i = 1}^n \frac{-\exp(-X_i\beta)}{(1 + \exp(-X_i\beta))^2} \cdot (-\tau_i)\\
&amp;= \frac{1}{n} \sum_{i = 1}^n \frac{\exp(-X_i\beta)}{(1 + \exp(-X_i\beta))^2} \cdot \tau_i
\end{align*}
<p>which is the first term of our jacobian.<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a></p>
<p>What about the other terms? Well, because the <span class="math inline">\(X_i\beta\)</span> term is additively separable, each term is the same, except for having a different term at the end. For example, the derivative with respect to <span class="math inline">\(\beta_2\)</span> is going to be</p>
<p><span class="math display">\[\frac{1}{n} \sum_{i = 1}^n \frac{\exp(-X_i\beta))}{(1 + \exp(-X_i\beta))^2} \cdot \text{age}_i\]</span>.</p>
<p>So the full jacobian is <span class="math display">\[
J = 
\left[
  \begin{array}{l} \\
    \frac{1}{n} \sum_{i = 1}^n \frac{\exp(-X_i\beta)}{(1 + \exp(-X_i\beta))^2} \cdot 1\\
    \frac{1}{n} \sum_{i = 1}^n \frac{\exp(-X_i\beta)}{(1 + \exp(-X_i\beta))^2} \cdot \tau_i\\
    \frac{1}{n} \sum_{i = 1}^n \frac{\exp(-X_i\beta)}{(1 + \exp(-X_i\beta))^2} \cdot \text{age}_i\\
    \frac{1}{n} \sum_{i = 1}^n \frac{\exp(-X_i\beta)}{(1 + \exp(-X_i\beta))^2} \cdot \tau_i \cdot \text{age}_i
  \end{array}
\right]
\]</span></p>
<p>This can be rewritten as</p>
<p><span class="math display">\[
J = 
\frac{1}{n}
\left[
  \begin{array} \\
     \frac{\exp(-X_1\beta)}{(1 + \exp(-X_1\beta))^2} &amp;
     \frac{\exp(-X_2\beta)}{(1 + \exp(-X_2\beta))^2} &amp;
     \cdots &amp;
     \frac{\exp(-X_n\beta)}{(1 + \exp(-X_n\beta))^2}
  \end{array}
\right]
\left[
  \begin{array}{cccc}\\
    1 &amp; \tau_1 &amp; \text{age}_1 &amp; \tau_1 \cdot \text{age}_1 \\
    1 &amp; \tau_2 &amp; \text{age}_2 &amp; \tau_2 \cdot \text{age}_2 \\
    \vdots  &amp; \vdots &amp; \vdots &amp; \vdots\\
    1 &amp; \tau_n &amp; \text{age}_n &amp; \tau_n \cdot \text{age}_n \\
  \end{array}
\right]
\]</span></p>
<p>The first term applies the derivative of the inverse link function to every linear predictor in the model, and the second term is the covariate matrix from our data! How convenient! That’s going to be true for all general linear models. So we can rewrite this as</p>
<p><span class="math display">\[J = \frac{\left[\begin{array}\\ \frac{\exp(-X_1\beta)}{(1 + \exp(-X_1\beta))^2} &amp; \frac{\exp(-X_2\beta)}{(1 + \exp(-X_2\beta))^2} &amp; \cdots &amp; \frac{\exp(-X_n\beta)}{(1 + \exp(-X_n\beta))^2}\end{array}\right] X}{n}\]</span></p>
<p>Recalling that our variance is <span class="math inline">\(J V J^T\)</span>, and that we’ve already calculated <span class="math inline">\(X_i\beta\)</span> for all <span class="math inline">\(i\)</span>, this is relatively simple to do in R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Get the data</span>
x &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(lg)

<span class="co"># CONTROL ERROR</span>

<span class="co"># Apply the derivative of the inverse link function to the linear predictors</span>
deriv &lt;-<span class="st"> </span><span class="kw">as.vector</span>(<span class="kw">exp</span>(-pred_ctl) /<span class="st"> </span>(<span class="dv">1</span> +<span class="st"> </span><span class="kw">exp</span>(-pred_ctl))^<span class="dv">2</span>)

<span class="co"># Set treatment to 0</span>
x[, <span class="st">'treatment'</span>] &lt;-<span class="st"> </span><span class="dv">0</span>
x[, <span class="st">'treatment:age'</span>] &lt;-<span class="st"> </span>x[, <span class="st">'treatment'</span>] *<span class="st"> </span>x[, <span class="st">'age'</span>]

<span class="co"># Complete the chain rule by matrix-multiplying the derivatives by the data,</span>
<span class="co"># now we have the jacobian</span>
j &lt;-<span class="st"> </span>deriv %*%<span class="st"> </span>x /<span class="st"> </span><span class="kw">nrow</span>(x)

<span class="co"># The variance of our estimate is the cross product of the jacobian and the model's</span>
<span class="co"># variance-covariance matrix</span>
variance &lt;-<span class="st"> </span>j %*%<span class="st"> </span><span class="kw">vcov</span>(lg) %*%<span class="st"> </span><span class="kw">t</span>(j)

<span class="co"># The error is the square root of that</span>
se_ctl &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">diag</span>(variance))

<span class="co"># TREATMENT ERROR: same logic</span>

deriv &lt;-<span class="st"> </span><span class="kw">as.vector</span>(<span class="kw">exp</span>(-pred_treat) /<span class="st"> </span>(<span class="dv">1</span> +<span class="st"> </span><span class="kw">exp</span>(-pred_treat))^<span class="dv">2</span>)

x &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(lg)

x[, <span class="st">'treatment'</span>] &lt;-<span class="st"> </span><span class="dv">1</span>
x[, <span class="st">'treatment:age'</span>] &lt;-<span class="st"> </span>x[, <span class="st">'treatment'</span>] *<span class="st"> </span>x[, <span class="st">'age'</span>]

j &lt;-<span class="st"> </span>deriv %*%<span class="st"> </span>x /<span class="st"> </span><span class="kw">nrow</span>(x)

variance &lt;-<span class="st"> </span>j %*%<span class="st"> </span><span class="kw">vcov</span>(lg) %*%<span class="st"> </span><span class="kw">t</span>(j)
se_treat &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">diag</span>(variance))

se_ctl</code></pre></div>
<pre><code>## [1] 0.009334252</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">se_treat</code></pre></div>
<pre><code>## [1] 0.009089667</code></pre>
</div>
<div id="review" class="section level2">
<h2>Review</h2>
<p>OK, we’ve got our estimates and variance:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">result &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">Label =</span> <span class="kw">c</span>(<span class="st">&quot;treatment = 0&quot;</span>, <span class="st">&quot;treatment = 1&quot;</span>),
  <span class="dt">Margin =</span> <span class="kw">c</span>(mean_pp_ctl, mean_pp_treat),
  <span class="dt">Standard.Error =</span> <span class="kw">c</span>(se_ctl, se_treat)
)

result</code></pre></div>
<pre><code>##           Label    Margin Standard.Error
## 1 treatment = 0 0.1126685    0.009334252
## 2 treatment = 1 0.2083750    0.009089667</code></pre>
<p>What do we get from modmarg?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">marg &lt;-<span class="st"> </span>modmarg::<span class="kw">marg</span>(<span class="dt">mod =</span> lg, <span class="dt">var_interest =</span> <span class="st">'treatment'</span>)
marg[[<span class="dv">1</span>]][, <span class="kw">c</span>(<span class="st">&quot;Label&quot;</span>, <span class="st">&quot;Margin&quot;</span>, <span class="st">&quot;Standard.Error&quot;</span>)]</code></pre></div>
<pre><code>##           Label    Margin Standard.Error
## 1 treatment = 0 0.1126685    0.009334252
## 2 treatment = 1 0.2083750    0.009089667</code></pre>
<p>Hooray!</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>“Usually” because this statement requires that the canonical link function for the regression has a closed-form derivative. Luckily, this is true for most common forms of linear regression.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>The exact form of the link function and its inverse will depend on the type of regression. For example, the logit function is the canonical link function for logistic regression and allows transformations between probabilities and log-odds.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Note that we treat the input <span class="math inline">\(X\)</span> as fixed and <span class="math inline">\(\beta\)</span> as a random variable.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>The Jacobian matrix is just the name of the matrix of all first partial derviatives of a vector-valued function.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>As a quick reminder about the variance-covariance matrix, if your model contains coefficients <span class="math inline">\(b_0\)</span> to <span class="math inline">\(b_n\)</span> (each with mean <span class="math inline">\(\mu_{b_i}\)</span> and standard deviation <span class="math inline">\(\sigma_{b_i}\)</span>), the <span class="math inline">\(i,j\)</span>th element of the variance-covariance matrix is <span class="math inline">\(cov(b_i, b_j)\)</span>.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>Quick reminder: for scalar <span class="math inline">\(a\)</span> and <span class="math inline">\(r\)</span>, where <span class="math inline">\(b\)</span> is the variance of random variable <span class="math inline">\(r\)</span>, <span class="math inline">\(\text{Var}(a \cdot r)\)</span> is <span class="math inline">\(a^2 \cdot b\)</span>. The matrix analog of <span class="math inline">\(a^2 \cdot \text{Var}(b)\)</span> is <span class="math inline">\(ABA^T\)</span>.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>Some practical notes on calculating the jacobian: the example at the bottom of the vignette captures how to calculate the jacobian for predicted levels pretty thoroughly. For predictive effects, the big change is that you are now calculating the variance on predicted effects, so you need to take the second derivative of the link function (instead of the first derivative). For categorical variables, you can take the second derivative by subtracting each of the levels from the base level and returning that as the jacobian. For continuous variables, you need to actually compute the second derivative of the link function and use that in place of the first derivative above. You need to explicitly compute the second derivative because you want an instantaneous rate of change as opposed to the rate of change over a range as with categorical variables above.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>Usually you’ll just want the variance-covariance matrix of the regression, but you’ll need to modify the standard variance-covariance matrix if you want to cluster standard errors or something.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>Note that the first part of the expression is sometimes written <span class="math inline">\(\frac{1}{1 + \exp(-X_i\beta)} \cdot \frac{1}{1 + \exp(X_i\beta)}\)</span> since that implies <span class="math inline">\(\frac{\partial}{\partial \beta} \text{logit}^{-1}(X\beta) = \text{logit}^{-1}(X\beta) \cdot \text{logit}^{-1}(-X\beta)\)</span><a href="#fnref9">↩</a></p></li>
</ol>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
